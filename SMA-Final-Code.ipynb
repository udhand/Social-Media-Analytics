{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries and setup HTTP connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import tqdm\n",
    "import networkx as nx\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import re\n",
    "\n",
    "client = ''\n",
    "secret = ''\n",
    "auth = requests.auth.HTTPBasicAuth(client, secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'grant_type': 'password',\n",
    "    'username': '',\n",
    "    'password': ''\n",
    "}\n",
    "\n",
    "headers = {'User-Agent': 'MyAPI/0.0.1'}\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token', auth=auth, data=data, headers=headers)\n",
    "token = res.json()['access_token']  \n",
    "headers = {**headers, **{'Authorization': f'bearer {token}'}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions to retrieve reddit posts and comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_posts(subreddit, headers, limit=100, max_posts=1000):\n",
    "    posts = []\n",
    "    after = None\n",
    "    while len(posts) < max_posts:\n",
    "        url = f'https://oauth.reddit.com/r/{subreddit}?limit={limit}'\n",
    "        if after:\n",
    "            url += f'&after={after}'\n",
    "        \n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            json_response = response.json()\n",
    "            posts.extend(json_response['data']['children'])\n",
    "            after = json_response['data'].get('after')\n",
    "            if not after:\n",
    "                break  # No more posts to fetch\n",
    "        else:\n",
    "            print(f\"Failed to fetch posts: {response.status_code}\")\n",
    "            break\n",
    "    \n",
    "    return posts\n",
    "\n",
    "def post_ids(posts):\n",
    "    return [post['data']['id'] for post in posts]\n",
    "\n",
    "def fetch_comments(subreddit,post_id, headers, limit=100, max_comments=1000):\n",
    "    comments = []\n",
    "    after = None\n",
    "    while len(comments) < max_comments:\n",
    "        url = f'https://oauth.reddit.com/r/{subreddit}/comments/{post_id}?limit={limit}'\n",
    "        if after:\n",
    "            url += f'&after={after}'\n",
    "        \n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            json_response = response.json()\n",
    "            comments.extend(json_response[1]['data']['children'])\n",
    "            after = json_response[1]['data'].get('after')\n",
    "            if not after:\n",
    "                break  # No more comments to fetch\n",
    "        else:\n",
    "            print(f\"Failed to fetch comments: {response.status_code}\")\n",
    "            time.sleep(1)\n",
    "            break\n",
    "    \n",
    "    return comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to combine posts and comments, for given subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final code to get post, poster_name, comments and commentor_name to a dataframe. post and comments in 'content' , user in 'user' and a flag for post or comment in 'type'\n",
    "\n",
    "def posts_to_df(subreddit, headers, limit=100, max_posts=1000):\n",
    "\n",
    "    df = pd.DataFrame(columns=['content', 'user', 'type','subreddit','post_id'])\n",
    "\n",
    "    posts = fetch_posts(subreddit, headers, limit=limit, max_posts=max_posts)\n",
    "\n",
    "    for post in posts:\n",
    "        new_row = pd.DataFrame({\n",
    "        'content': [post['data']['title']],\n",
    "        'user': [post['data']['author']],\n",
    "        'type': ['post'],\n",
    "        'subreddit': [subreddit],\n",
    "        'post_id': [post['data']['id']]\n",
    "        })\n",
    "        df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "        comments = fetch_comments(subreddit,post['data']['id'], headers, limit=limit, max_comments=max_posts)\n",
    "        \n",
    "        for comment in comments:\n",
    "            \n",
    "            if 'body' not in comment['data']:\n",
    "                continue\n",
    "            \n",
    "            new_row = pd.DataFrame({\n",
    "            'content': [comment['data']['body']],\n",
    "            'user': [comment['data']['author']],\n",
    "            'type': ['comment'],\n",
    "            'subreddit': [subreddit],\n",
    "            'post_id': [post['data']['id']]\n",
    "            })  \n",
    "            df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [1:28:01<00:00, 1320.44s/it]\n"
     ]
    }
   ],
   "source": [
    "subreddits = ['democrats', 'republican','conservative','liberal']\n",
    "\n",
    "posts_df = pd.DataFrame(columns=['content', 'user', 'type','subreddit','post_id'])\n",
    "\n",
    "for subreddit in tqdm.tqdm(subreddits):\n",
    "    posts_df_ = posts_to_df(subreddit, headers, limit=100, max_posts=5000)\n",
    "    time.sleep(1000)\n",
    "    posts_df = pd.concat([posts_df, posts_df_], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip data download, and load from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#posts_df.to_csv('reddit_posts.csv')\n",
    "#posts_df = pd.read_csv('reddit_posts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/uditdhand/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/uditdhand/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "posts_df_['content'] = posts_df_['content'].str.lower()\n",
    "posts_df_t = posts_df_[(posts_df_['content'].str.contains('trump'))&~(posts_df_['content'].str.contains('biden'))]\n",
    "posts_df_b = posts_df_[(posts_df_['content'].str.contains('biden'))&~(posts_df_['content'].str.contains('trump'))]\n",
    "\n",
    "posts_df_ = pd.concat([posts_df_t,posts_df_b])\n",
    "\n",
    "posts_df = posts_df_[posts_df_['type']=='post'].groupby('subreddit').apply(lambda x: x.sample(100,random_state=1))\n",
    "posts_df.reset_index(drop=True,inplace=True)\n",
    "def about_trump_or_biden(text):\n",
    "    if 'trump' in text:\n",
    "        return 'trump'\n",
    "    elif 'biden' in text:\n",
    "        return 'biden'\n",
    "\n",
    "posts_df['about_trump_or_biden'] = posts_df['content'].apply(about_trump_or_biden)\n",
    "\n",
    "# before topic modelling remove stop words and punctuation, and custom words\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['donald','joe','trump','biden','http','https','www','com','republican','democrat','republicans','democrats','president','election','vote','voting','voted','voter','votes','voters'])\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = word_tokenize(text)\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if word not in stop_words]\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "\n",
    "posts_df['cleaned_content'] = posts_df['content'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    try:\n",
    "        result = sentiment_pipeline(text)\n",
    "        return result[0]['label'], result[0]['score']\n",
    "    except Exception as e:\n",
    "        return \"ERROR\", None\n",
    "\n",
    "posts_df[['sentiment', 'score']] = posts_df.apply(lambda row: analyze_sentiment(row['content']), axis=1, result_type='expand')\n",
    "\n",
    "# count of positive and negative sentiments for trump and biden and for  each subreddit \n",
    "\n",
    "counts = posts_df.groupby(['subreddit','about_trump_or_biden','sentiment']).size().unstack(fill_value=0)\n",
    "counts['TOTAL'] = counts['NEGATIVE'] + counts['POSITIVE']\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "model = BERTopic(language=\"english\", calculate_probabilities=True, verbose=True)\n",
    "topics, probs = model.fit_transform(posts_df['cleaned_content'])\n",
    "\n",
    "# Create a DataFrame to hold the topic and probability information\n",
    "topics_df = pd.DataFrame({\n",
    "    'topic': topics,\n",
    "    'probability': [max(prob) if prob.size > 0 else None for prob in probs]\n",
    "})\n",
    "\n",
    "# You can now inspect the topics_df DataFrame\n",
    "print(topics_df.head())\n",
    "\n",
    "topics_df['topic_name'] = topics_df['topic'].apply(lambda x: model.get_topic_info(x)['Name'])\n",
    "posts_df['topic'] = topics_df['topic_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of topic wise posts , sentiment wise, trump or biden wise, subreddit wise\n",
    "\n",
    "counts = posts_df.groupby(['subreddit','about_trump_or_biden','sentiment','topic']).size().unstack(fill_value=0)\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "avg_sentiment = posts_df.groupby(['topic', 'user'])['score'].mean().reset_index()\n",
    "\n",
    "edges = [(row['user'], row['topic'], row['score']) for index, row in avg_sentiment.iterrows()]\n",
    "\n",
    "user_subreddit = posts_df[['user', 'subreddit']].drop_duplicates().set_index('user')['subreddit'].to_dict()\n",
    "topic_nodes = avg_sentiment['topic'].unique()\n",
    "user_nodes = avg_sentiment['user'].unique()\n",
    "\n",
    "color_map = {'republican': 'red', 'democrats': 'blue', 'liberal': 'blue', 'conservative': 'red'}\n",
    "node_color_map = {**{topic: 'yellow' for topic in topic_nodes}, **color_map}  # Assuming topics have a default color, e.g., 'yellow'\n",
    "all_nodes = list(topic_nodes) + list(user_nodes)\n",
    "node_colors = [node_color_map[user_subreddit.get(user, 'default')] if user in user_nodes else 'yellow' for user in all_nodes]\n",
    "B = nx.Graph()\n",
    "B.add_nodes_from(topic_nodes, bipartite=0)\n",
    "B.add_nodes_from(user_nodes, bipartite=1)\n",
    "B.add_weighted_edges_from(edges)\n",
    "\n",
    "pos = nx.spring_layout(B)\n",
    "\n",
    "nx.draw(B, pos, with_labels=False, node_color=node_colors, edge_color='black', node_size=50, font_size=3, edge_cmap=plt.cm.Blues, width=0.3, alpha=0.7)\n",
    "\n",
    "# label only topics\n",
    "topic_nodes = {node: node for node in topic_nodes}\n",
    "nx.draw_networkx_labels(B, pos, labels=topic_nodes, font_size=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Network metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centrality measures of the topics:\n",
    "\n",
    "degree_centrality = nx.degree_centrality(B)\n",
    "closeness_centrality = nx.closeness_centrality(B)\n",
    "betweenness_centrality = nx.betweenness_centrality(B)\n",
    "\n",
    "centrality_measures = pd.DataFrame({\n",
    "    'topic': list(degree_centrality.keys()),\n",
    "    'degree_centrality': list(degree_centrality.values()),\n",
    "    'closeness_centrality': list(closeness_centrality.values()),\n",
    "    'betweenness_centrality': list(betweenness_centrality.values())\n",
    "})\n",
    "\n",
    "centrality_measures.sort_values(by='betweenness_centrality', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Community detection and affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import community as community_louvain  # This library is often called python-louvain\n",
    "\n",
    "# Assuming you have your posts_df DataFrame from the previous context\n",
    "avg_sentiment = posts_df.groupby(['topic', 'user'])['score'].mean().reset_index()\n",
    "\n",
    "edges = [(row['user'], row['topic'], row['score']) for index, row in avg_sentiment.iterrows()]\n",
    "\n",
    "user_subreddit = posts_df[['user', 'subreddit']].drop_duplicates().set_index('user')['subreddit'].to_dict()\n",
    "topic_nodes = avg_sentiment['topic'].unique()\n",
    "user_nodes = avg_sentiment['user'].unique()\n",
    "\n",
    "B = nx.Graph()\n",
    "B.add_nodes_from(topic_nodes, bipartite=0)\n",
    "B.add_nodes_from(user_nodes, bipartite=1)\n",
    "B.add_weighted_edges_from(edges)\n",
    "\n",
    "# Detecting communities\n",
    "partition = community_louvain.best_partition(B)\n",
    "\n",
    "# Printing users in each community\n",
    "for community_id in set(partition.values()):\n",
    "    print(f\"Community {community_id}:\")\n",
    "    members = [node for node, community in partition.items() if community == community_id and node in user_nodes]\n",
    "    print(members)\n",
    "\n",
    "# now find affiliation of each community with trump or biden\n",
    "\n",
    "community_affiliation = {}\n",
    "for community_id in set(partition.values()):\n",
    "    members = [node for node, community in partition.items() if community == community_id and node in user_nodes]\n",
    "    community_affiliation[community_id] = posts_df[posts_df['user'].isin(members)]['subreddit'].value_counts().idxmax()\n",
    "\n",
    "community_affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what are the differences between 3  'liberal' communities that got detected?\n",
    "\n",
    "communities = [community_id for community_id, affiliation in community_affiliation.items() if affiliation=='republican']\n",
    "\n",
    "for community_id in communities:\n",
    "    members = [node for node, community in partition.items() if community == community_id and node in user_nodes]\n",
    "    print(f\"Community {community_id}:\")\n",
    "    print(posts_df[posts_df['user'].isin(members)]['topic'].value_counts().head(5))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
